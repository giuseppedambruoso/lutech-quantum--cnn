{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from math import sqrt\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import Compose, Resize, Grayscale, ToTensor\n",
    "from torch.nn import Sequential, Conv2d, BatchNorm2d, ReLU, MaxPool2d, functional\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define constants and preprocessing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train dataset from class-specific folders\n",
    "dataset = ImageFolder(\n",
    "    root=r\"C:\\Users\\giuseppe.dambruoso\\OneDrive - LUTECH SPA\\Desktop\\Progetto\\TetrisDataset\\Train\",\n",
    "    # Compose a sequence of transformations to apply to each image\n",
    "    transform=Compose(\n",
    "        [\n",
    "            # Convert the image to grayscale (single channel)\n",
    "            Grayscale(num_output_channels=1),\n",
    "            # Convert the image to a PyTorch tensor\n",
    "            ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    # Compose a sequence of transformations to apply to each target label\n",
    "    target_transform=Compose(\n",
    "        [\n",
    "            # Convert the target labels to a PyTorch tensor\n",
    "            lambda x: torch.tensor(x),\n",
    "            # One-hot encode the target labels\n",
    "            lambda x: torch.eye(NUM_CLASSES)[x].to(torch.float64),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create two subsets randomly\n",
    "# subset_indices_train = torch.randperm(len(dataset))[:700]\n",
    "subset_indices_val = torch.randperm(len(dataset))[:100]\n",
    "\n",
    "# subset_train = Subset(dataset, subset_indices_train)\n",
    "subset_val = Subset(dataset, subset_indices_val)\n",
    "\n",
    "# # Create DataLoader for each subset\n",
    "# train_loader = DataLoader(\n",
    "#     subset_train,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False,\n",
    "#     drop_last=True\n",
    "# )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    Subset(dataset, [0]), batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    subset_val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train dataset from class-specific folders\n",
    "test_ds = ImageFolder(\n",
    "    root=r\"C:\\Users\\giuseppe.dambruoso\\OneDrive - LUTECH SPA\\Desktop\\Progetto\\TetrisDataset\\Test\",\n",
    "    # Compose a sequence of transformations to apply to each image\n",
    "    transform=Compose(\n",
    "        [\n",
    "            # Convert the image to grayscale (single channel)\n",
    "            Grayscale(num_output_channels=1),\n",
    "            # Convert the image to a PyTorch tensor\n",
    "            ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    # Compose a sequence of transformations to apply to each target label\n",
    "    target_transform=Compose(\n",
    "        [\n",
    "            # Convert the target labels to a PyTorch tensor\n",
    "            lambda x: torch.tensor(x),\n",
    "            # One-hot encode the target labels\n",
    "            lambda x: torch.eye(NUM_CLASSES)[x].to(torch.float64),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create DataLoader for each subset\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show some images from the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure_width = 15\n",
    "# figure_height = 3\n",
    "# num_images_to_display = 7\n",
    "\n",
    "# for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "#     plt.figure(figsize=(figure_width, figure_height))\n",
    "\n",
    "#     for i in range(num_images_to_display):\n",
    "#         plt.subplot(1, num_images_to_display, i + 1)\n",
    "#         image = np.squeeze(data[i].numpy())\n",
    "#         plt.imshow(image, cmap='gray')\n",
    "#         plt.title(f\"Class: {torch.argmax(targets[i]).item()}\")\n",
    "#         plt.axis('off')\n",
    "#     plt.suptitle(\"SOME IMAGES FROM THE TRAIN DATASET\", fontsize=14)\n",
    "#     plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show some images from the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, (data, targets) in enumerate(validation_loader):\n",
    "#     plt.figure(figsize=(figure_width, figure_height))\n",
    "\n",
    "#     for i in range(num_images_to_display):\n",
    "#         plt.subplot(1, num_images_to_display, i + 1)\n",
    "#         image = np.squeeze(data[i].numpy())\n",
    "#         plt.imshow(image, cmap='gray')\n",
    "#         plt.title(f\"Class: {torch.argmax(targets[i]).item()}\")\n",
    "#         plt.axis('off')\n",
    "#     plt.suptitle(\"SOME IMAGES FROM THE VALIDATION DATASET\", fontsize=14)\n",
    "#     plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show some images from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, (data, targets) in enumerate(validation_loader):\n",
    "#     plt.figure(figsize=(figure_width, figure_height))\n",
    "\n",
    "#     for i in range(num_images_to_display):\n",
    "#         plt.subplot(1, num_images_to_display, i + 1)\n",
    "#         image = np.squeeze(data[i].numpy())\n",
    "#         plt.imshow(image, cmap='gray')\n",
    "#         plt.title(f\"Class: {torch.argmax(targets[i]).item()}\")\n",
    "#         plt.axis('off')\n",
    "#     plt.suptitle(\"SOME IMAGES FROM THE VALIDATION DATASET\", fontsize=14)\n",
    "#     plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the distribution of the images among the classes in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to calculate class distribution\n",
    "# def calculate_class_distribution(loader):\n",
    "#     class_counts = [0] * NUM_CLASSES\n",
    "#     total_samples = 0\n",
    "\n",
    "#     for _, labels in loader:\n",
    "#         for label in labels:\n",
    "#             class_counts[label.argmax().item()] += 1\n",
    "#             total_samples += 1\n",
    "\n",
    "#     class_distribution = [count / total_samples for count in class_counts]\n",
    "\n",
    "#     return class_distribution\n",
    "\n",
    "# # Calculate class distribution for training dataset\n",
    "# train_class_distribution = calculate_class_distribution(train_loader)\n",
    "\n",
    "# # Calculate class distribution for validation dataset\n",
    "# val_class_distribution = calculate_class_distribution(validation_loader)\n",
    "\n",
    "# # Calculate class distribution for validation dataset\n",
    "# test_class_distribution = calculate_class_distribution(test_loader)\n",
    "\n",
    "\n",
    "# # Class labels\n",
    "# class_labels = ['S', 'L', 'O', 'T']\n",
    "\n",
    "# # Plot pie chart for training dataset\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.pie(train_class_distribution, labels=class_labels, autopct='%1.1f%%')\n",
    "# plt.title('Training Dataset')\n",
    "\n",
    "# # Plot pie chart for validation dataset\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.pie(val_class_distribution, labels=class_labels, autopct='%1.1f%%')\n",
    "# plt.title('Validation Dataset')\n",
    "\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.pie(test_class_distribution, labels=class_labels, autopct='%1.1f%%')\n",
    "# plt.title('Test Dataset')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train dataset from class-specific folders\n",
    "test_ds = ImageFolder(\n",
    "    root=r\"C:\\Users\\giuseppe.dambruoso\\OneDrive - LUTECH SPA\\Desktop\\Progetto\\TetrisDataset\\Test\",\n",
    "    # Compose a sequence of transformations to apply to each image\n",
    "    transform=Compose(\n",
    "        [\n",
    "            # Convert the image to grayscale (single channel)\n",
    "            Grayscale(num_output_channels=1),\n",
    "            # Convert the image to a PyTorch tensor\n",
    "            ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    # Compose a sequence of transformations to apply to each target label\n",
    "    target_transform=Compose(\n",
    "        [\n",
    "            # Convert the target labels to a PyTorch tensor\n",
    "            lambda x: torch.tensor(x),\n",
    "            # One-hot encode the target labels\n",
    "            lambda x: torch.eye(NUM_CLASSES)[x].to(torch.float64),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create DataLoader for each subset\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the convolutional neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the quantum convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the hyperparameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the functions to perform training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, loss_fn, optimizer):\n",
    "    \"\"\"Performs one training epoch.\"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_index, (inputs, labels) in enumerate(data_loader):\n",
    "        # print('batch index: ', batch_index+1)\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs.float(), labels.float())\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        _, true_labels = torch.max(labels, 1)\n",
    "        correct_predictions = (predicted_labels == true_labels).sum().item()\n",
    "        accuracy = correct_predictions / BATCH_SIZE * 100\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = (end_time - start_time) / 60\n",
    "    return loss, accuracy, epoch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_fn):\n",
    "    \"\"\"Performs evaluation of the model.\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (inputs, labels) in enumerate(data_loader):\n",
    "            # print('batch index: ', batch_idx+1)\n",
    "            outputs = model(inputs)\n",
    "            total_loss += loss_fn(outputs.float(), labels.float()).item()\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            _, true_labels = torch.max(labels, 1)\n",
    "            correct_predictions += (predicted_labels == true_labels).sum().item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / len(data_loader.dataset) * 100\n",
    "    end_time = time.time()\n",
    "    evaluation_time = (end_time - start_time) / 60\n",
    "    return avg_loss, accuracy, evaluation_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for storing CSV\n",
    "csv_file_path = r\"C:\\Users\\giuseppe.dambruoso\\OneDrive - LUTECH SPA\\Desktop\\Progetto\\Risultati\\Tetris\\QuantConv\\training_metrics.csv\"\n",
    "\n",
    "# Write the header to the CSV file\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"Epoch\",\n",
    "            \"Train Loss\",\n",
    "            \"Train Accuracy\",\n",
    "            \"Validation Loss\",\n",
    "            \"Validation Accuracy\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Choose the number of epochs\n",
    "EPOCHS = 1000\n",
    "\n",
    "# Define the lists to store training and validation metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "validation_losses = []\n",
    "validation_accuracies = []\n",
    "model_state_dicts = []\n",
    "\n",
    "# Perform training and validation\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH {}\".format(epoch + 1))\n",
    "    # Perform one training epoch\n",
    "    train_loss, train_accuracy, train_time = train_one_epoch(\n",
    "        model, train_loader, loss_fn, optimizer\n",
    "    )\n",
    "    train_losses.append(train_loss.item())\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    print(\n",
    "        \"TRAIN: loss {:.3f}; accuracy {:.2f}%; time {:.2f}min\".format(\n",
    "            train_loss.item(), train_accuracy, train_time\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add model state dict to the list\n",
    "    model_state_dicts.append(model.state_dict())\n",
    "\n",
    "    # Perform validation\n",
    "    validation_loss, validation_accuracy, validation_time = evaluate(\n",
    "        model, validation_loader, loss_fn\n",
    "    )\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "    print(\n",
    "        \"VALIDATION: loss {:.3f}; accuracy {:.3f}%; time {:.2f}min\".format(\n",
    "            validation_loss, validation_accuracy, validation_time\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Write the current epoch's metrics to the CSV file\n",
    "    with open(csv_file_path, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(\n",
    "            [\n",
    "                epoch + 1,\n",
    "                train_loss.item(),\n",
    "                train_accuracy,\n",
    "                validation_loss,\n",
    "                validation_accuracy,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the training convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(train_losses, label=\"Loss\", color=\"blue\")\n",
    "plt.plot(np.array(train_accuracies) / 100, label=\"Accuracy\", color=\"orange\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Metrics\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show validation convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(validation_losses, label=\"Loss\", color=\"blue\")\n",
    "plt.plot(np.array(validation_accuracies) / 100, label=\"Accuracy\", color=\"orange\")\n",
    "plt.ylabel(\"Validation Metrics\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print best model index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_idx = validation_accuracies.index(max(validation_accuracies))\n",
    "print(\"best model obtained at the {}th epoch.\".format(best_model_idx + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\giuseppe.dambruoso\\OneDrive - LUTECH SPA\\Desktop\\Progetto\\Risultati\\Tetris\\QuantConv\\BestModel\"\n",
    "torch.save(\n",
    "    model_state_dicts[validation_accuracies.index(max(validation_accuracies))], path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_accuracy, _ = evaluate(model, test_loader, loss_fn)\n",
    "print(\"Accuracy on test set: {:.2f}% \".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
