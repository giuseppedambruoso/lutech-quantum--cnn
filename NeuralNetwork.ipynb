{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision.transforms import Compose, Resize, Grayscale, ToTensor\n",
    "from torch.nn import Sequential, Conv2d, BatchNorm2d, ReLU, MaxPool2d, functional\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.primitives import BackendEstimator, BackendSampler\n",
    "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN, SamplerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants and preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_SIZE = (160, 160)  # Image size for resizing\n",
    "NUM_CLASSES = 4  # Number of classes in the dataset\n",
    "BATCH_SIZE = 32  # Batch size for data loading\n",
    "\n",
    "# Preprocessing function (to be used only with QNN)\n",
    "def scale_image(image):\n",
    "    \"\"\"Scale the pixel values so that they are in [0, pi]\"\"\"\n",
    "    image = np.array(image.squeeze())\n",
    "    scaler = StandardScaler()\n",
    "    scaled_img= scaler.fit_transform(image)\n",
    "    scaled_img = scaled_img/max(scaled_img.flatten()) *np.pi/2 + np.pi/2\n",
    "    \n",
    "    return torch.tensor(scaled_img).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train dataset from class-specific folders\n",
    "train_ds = ImageFolder(\n",
    "    root=r\"C:\\Users\\giuseppe.dambruoso\\OneDrive - LUTECH SPA\\Desktop\\Progetto\\Dataset\\train\",\n",
    "    # Compose a sequence of transformations to apply to each image\n",
    "    transform=Compose([\n",
    "        # Resize the image to IMG_SIZE\n",
    "        Resize(IMG_SIZE),\n",
    "        # Convert the image to grayscale (single channel)\n",
    "        Grayscale(num_output_channels=1),\n",
    "        # Convert the image to a PyTorch tensor\n",
    "        ToTensor(),\n",
    "        # Rescale pixel values so that they are between 0 and pi\n",
    "        lambda x: scale_image(x)\n",
    "    ]),\n",
    "    # Compose a sequence of transformations to apply to each target label\n",
    "    target_transform=Compose([\n",
    "        # Convert the target labels to a PyTorch tensor\n",
    "        lambda x: torch.tensor(x),\n",
    "        # One-hot encode the target labels \n",
    "        lambda x: torch.eye(NUM_CLASSES)[x].to(torch.float64)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Load dataset in batches\n",
    "train_loader = DataLoader(\n",
    "    train_ds,  # Training dataset\n",
    "    batch_size=BATCH_SIZE,  # Batch size for loading data\n",
    "    shuffle=True,  # Shuffle the data at every epoch\n",
    "    drop_last=True  # Drop the last batch if it is incomplete\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some images from the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_width = 15\n",
    "figure_height = 3\n",
    "num_images_to_display = 7\n",
    "\n",
    "for batch_idx, (data, targets) in enumerate(train_loader):  \n",
    "    plt.figure(figsize=(figure_width, figure_height))\n",
    "    \n",
    "    for i in range(num_images_to_display):\n",
    "        plt.subplot(1, num_images_to_display, i + 1)\n",
    "        image = np.squeeze(data[i].numpy())\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f\"Class: {torch.argmax(targets[i]).item()}\")\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(\"SOME IMAGES FROM THE TRAIN DATASET\", fontsize=14)\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from class-specific folders\n",
    "validation_ds = ImageFolder(\n",
    "    root=r\"C:\\Users\\giuseppe.dambruoso\\OneDrive - LUTECH SPA\\Desktop\\Progetto\\Dataset\\test\",  # Path to the validation data directory\n",
    "    # Compose a sequence of transformations to apply to each image\n",
    "    transform=Compose([\n",
    "        # Resize the image to IMG_SIZE\n",
    "        Resize(IMG_SIZE),\n",
    "        # Convert the image to grayscale (single channel)\n",
    "        Grayscale(num_output_channels=1),\n",
    "        # Convert the image to a PyTorch tensor\n",
    "        ToTensor(),\n",
    "        # Rescale pixel values so that they are between 0 and pi\n",
    "        lambda x: scale_image(x)\n",
    "    ]),\n",
    "    # Compose a sequence of transformations to apply to each target\n",
    "    target_transform=Compose([\n",
    "        # Convert the target to a PyTorch tensor\n",
    "        lambda x: torch.tensor(x),\n",
    "        # One-hot encode the target labels\n",
    "        lambda x: torch.eye(NUM_CLASSES)[x].to(torch.float64)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Load dataset in batches\n",
    "validation_loader = DataLoader(\n",
    "    validation_ds,  # Validation dataset\n",
    "    batch_size=BATCH_SIZE,  # Batch size for loading data\n",
    "    shuffle=True,  # Shuffle the data at every epoch\n",
    "    drop_last=True  # Drop the last batch if it is incomplete\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some images from the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data, targets) in enumerate(validation_loader):  \n",
    "    plt.figure(figsize=(figure_width, figure_height))\n",
    "    \n",
    "    for i in range(num_images_to_display):\n",
    "        plt.subplot(1, num_images_to_display, i + 1)\n",
    "        image = np.squeeze(data[i].numpy())\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f\"Class: {torch.argmax(targets[i]).item()}\")\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(\"SOME IMAGES FROM THE VALIDATION DATASET\", fontsize=14)\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the distribution of the images among the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate class distribution\n",
    "def calculate_class_distribution(loader):\n",
    "    class_counts = [0] * NUM_CLASSES\n",
    "    total_samples = 0\n",
    "    \n",
    "    for _, labels in loader:\n",
    "        for label in labels:\n",
    "            class_counts[label.argmax().item()] += 1\n",
    "            total_samples += 1\n",
    "            \n",
    "    class_distribution = [count / total_samples for count in class_counts]\n",
    "    \n",
    "    return class_distribution\n",
    "\n",
    "# Calculate class distribution for training dataset\n",
    "train_class_distribution = calculate_class_distribution(train_loader)\n",
    "\n",
    "# Calculate class distribution for validation dataset\n",
    "validation_class_distribution = calculate_class_distribution(validation_loader)\n",
    "\n",
    "# Class labels\n",
    "class_labels = ['Class 1', 'Class 2', 'Class 3', 'Class 4']\n",
    "\n",
    "# Plot pie chart for training dataset\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(train_class_distribution, labels=class_labels, autopct='%1.1f%%')\n",
    "plt.title('Training Dataset')\n",
    "\n",
    "# Plot pie chart for validation dataset\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(validation_class_distribution, labels=class_labels, autopct='%1.1f%%')\n",
    "plt.title('Validation Dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the convolutional neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the convolutional block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(in_channels, out_channels):\n",
    "    \"\"\"\n",
    "    Defines a convolutional block.\n",
    "\n",
    "    Args:\n",
    "        in_channels : The number of input channels.\n",
    "        out_channels : The number of output channels.\n",
    "\n",
    "    Returns:\n",
    "        A sequential module containing convolutional, ReLU, batch \n",
    "        normalization and max pooling layers.\n",
    "    \"\"\"\n",
    "    model_cb = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding='same'\n",
    "        ),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.BatchNorm2d(out_channels),\n",
    "        torch.nn.MaxPool2d(kernel_size=2)\n",
    "    )\n",
    "    return model_cb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the dense block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseBlock(in_features, out_features):\n",
    "    \"\"\"\n",
    "    Defines a convolutional block.\n",
    "\n",
    "    Args:\n",
    "        in_features : The number of input features.\n",
    "        out_features : The number of output features.\n",
    "\n",
    "    Returns:\n",
    "        A sequential module containing linear, ReLU, batch normalization\n",
    "        and dropout layers.\n",
    "    \"\"\"\n",
    "    model_db = torch.nn.Sequential(\n",
    "        torch.nn.Linear(\n",
    "            in_features,\n",
    "            out_features\n",
    "        ),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.BatchNorm1d(out_features),\n",
    "        torch.nn.Dropout(p=0.2)\n",
    "    )\n",
    "    return model_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the quantum convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parametrized quantum circuit used in the quantum filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of qubits that will compose the circuit\n",
    "N_QUBITS = 4\n",
    "\n",
    "# Create the feature map and ansatz circuits\n",
    "feature_map = ZZFeatureMap(N_QUBITS, parameter_prefix='x')\n",
    "ansatz = RealAmplitudes(N_QUBITS, reps=N_QUBITS, entanglement='linear')\n",
    "\n",
    "# Create the quantum circuit\n",
    "pqc = QuantumCircuit(N_QUBITS)\n",
    "\n",
    "# Add feature map and ansatz to the quantum circuit\n",
    "pqc.compose(feature_map, inplace=True)\n",
    "pqc.compose(ansatz, inplace=True)\n",
    "\n",
    "# Draw the quantum circuit\n",
    "pqc.draw(output='mpl', style='clifford')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the quantum filter used in the quantum convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantum_filter(input, pqc, backend, shots):\n",
    "    \"\"\"\n",
    "    Applies a quantum filter to a given 2x2 window of the input image.\n",
    "    The values of the window become the input parameters of a 4-qubit\n",
    "    quantum circuit. The quantum circuit is then executed shots times\n",
    "    on the specified backend. Finally, the average measurement outcome\n",
    "    is returned as output.\n",
    "\n",
    "    The execution of the circuit is performed by means of EstimatorQNN,\n",
    "    which takes also care of all the backpropagation procedures like\n",
    "    the computation of gradients as well as the update of the trainable\n",
    "    parameters of the circuit.\n",
    "\n",
    "    The compatibility between EstimatorQNN and PyTorch is handled by a\n",
    "    TorchConnector.\n",
    "\n",
    "    Args:\n",
    "        input: The window to which the quantum filter will be applied.\n",
    "        pqc: The quantum circuit associated to the quantum filter.\n",
    "        backend: The device or simulator that executes the circuit.\n",
    "        shots: The number of times the circuit will be executed.\n",
    "    \n",
    "    Returns:\n",
    "        The average result of the quantum circuit execution.\n",
    "    \"\"\"\n",
    "    qc_executer = EstimatorQNN(\n",
    "                    # Set quantum circuit\n",
    "                    circuit=pqc.decompose(),\n",
    "                    # Set the estimator that will execute the circuit\n",
    "                    estimator=BackendEstimator(backend),\n",
    "                    # Set non-trainable inpu parameters\n",
    "                    input_params=feature_map.parameters,\n",
    "                     # Set trainable parameters\n",
    "                    weight_params=ansatz.parameters,\n",
    "                    # Compute gradients with respect to input data for a\n",
    "                    # proper gradient computation with TorchConnector.\n",
    "                    input_gradients=True\n",
    "                )\n",
    "\n",
    "    # Set number of shots for the quantum circuit execution\n",
    "    qc_executer.estimator.set_options(shots=shots)\n",
    "\n",
    "    # Make the qc_executer compatible with PyTorch\n",
    "    quantum_filter = TorchConnector(qc_executer)\n",
    "    \n",
    "    # Apply the quantum filter to the flattened input window\n",
    "    output = quantum_filter(input.reshape(-1)).item()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the quantum convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantum_convolution(data_loader, pqc, backend, shots):\n",
    "   \"\"\"\n",
    "    Applies quantum convolution to each image of a specified data loader. \n",
    "    It operates by applying a quantum filter to every 2x2 sliding block\n",
    "    extracted from the image.\n",
    "\n",
    "    Args:\n",
    "        data_loader: The data loader containing the images.\n",
    "        pqc: The quantum circuit associated with the quantum filter.\n",
    "        backend: The backend associated with the quantum filter.\n",
    "        shots: The number of shots associated with the quantum filter.\n",
    "    \n",
    "    Returns:\n",
    "        The set of the convoluted images.\n",
    "   \"\"\"\n",
    "   # Unfold each image of tha data loader into 2x2 blocks\n",
    "   input_unfolded = torch.nn.functional.unfold(\n",
    "      data_loader, kernel_size=2, stride=2, padding=0).permute(0, 2, 1)\n",
    "\n",
    "    # Apply the quantum filter to each 2x2 block\n",
    "   output_unfolded = [[] for _ in range(input_unfolded.size(0))]   \n",
    "   for i in range(input_unfolded.size(0)):\n",
    "      for j in range(input_unfolded.size(1)):\n",
    "         output_unfolded[i].append(\n",
    "            quantum_filter(input_unfolded[i, j], pqc, backend, shots))\n",
    "   output_unfolded = torch.tensor(output_unfolded)\n",
    "    \n",
    "    # Refold the output images\n",
    "   output = output_unfolded.view(\n",
    "      output_unfolded.size(0), input.size(1), \n",
    "      int(sqrt(input_unfolded.size(1))), int(sqrt(input_unfolded.size(1))))\n",
    "   return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, backend, shots):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Initialize backend and shots attributes\n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "\n",
    "        # Define the convolutional neural network layers\n",
    "        self.cnn = torch.nn.Sequential(\n",
    "            ConvBlock(1, 256),\n",
    "            ConvBlock(256, 128),\n",
    "            ConvBlock(128, 64),\n",
    "            torch.nn.Flatten(),\n",
    "            DenseBlock(64*(IMG_SIZE[0]//8)**2, 128),\n",
    "            DenseBlock(128, 64),\n",
    "            torch.nn.Linear(64, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = quantum_convolution(x) # Apply quantum convolution\n",
    "        x = self.cnn(x) # Apply classical CNN\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the functions to perform training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, loss_fn, optimizer):\n",
    "    \"\"\"Performs one training epoch.\"\"\"   \n",
    "    model.train() # Set the model to training mode\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_index, (inputs, labels) in enumerate(data_loader):\n",
    "        optimizer.zero_grad() # Clear previous gradients\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Update model parameters\n",
    "\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        _, true_labels = torch.max(labels, 1)\n",
    "        correct_predictions = (predicted_labels == true_labels).sum().item()   \n",
    "        accuracy = correct_predictions / BATCH_SIZE * 100\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_time = (end_time - start_time)//60\n",
    "    return loss, accuracy, epoch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_fn):\n",
    "    \"\"\"Performs evaluation of the model.\"\"\"\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    total_loss = 0.0 \n",
    "    correct_predictions = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (inputs, labels) in enumerate(data_loader):\n",
    "            outputs = model(inputs)\n",
    "            total_loss += loss_fn(outputs, labels).item()\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            _, true_labels = torch.max(labels, 1)\n",
    "            correct_predictions += (\n",
    "                predicted_labels == true_labels).sum().item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / len(data_loader.dataset) * 100\n",
    "    end_time = time.time()\n",
    "    evaluation_time = (end_time - start_time)//60\n",
    "    return avg_loss, accuracy, evaluation_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the of times the quantum circuits will be executed\n",
    "SHOTS = 1000\n",
    "\n",
    "# Choose the backend on which the quantum circuits will be executed\n",
    "backend = Aer.get_backend('qasm_simulator')\n",
    "\n",
    "# Instantiate the convolutional neural network\n",
    "model = Net(backend, SHOTS)\n",
    "\n",
    "# Choose the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Choose the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Choose the number of epochs\n",
    "EPOCHS = 30\n",
    "\n",
    "# Define the lists to store training and validation metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "validation_losses = []\n",
    "validation_accuracies = []\n",
    "model_state_dicts = []\n",
    "\n",
    "# Perform training and validation\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH {epoch + 1}:')\n",
    "\n",
    "    # Perform one training epoch\n",
    "    train_loss, train_accuracy, train_time = train_one_epoch(\n",
    "        model, train_loader, loss_fn, optimizer)\n",
    "    train_losses.append(train_loss.item())\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    print('TRAIN: loss {:.3f}; accuracy {:.2f}%; time {:.0f}s'.format(\n",
    "          train_loss.item(), train_accuracy, train_time))\n",
    "\n",
    "    # Add model state dict to the list\n",
    "    model_state_dicts.append(model.state_dict())\n",
    "\n",
    "    # Perform validation\n",
    "    validation_loss, validation_accuracy, validation_time = evaluate(\n",
    "        model, validation_loader, loss_fn) \n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "    print('VALIDATION: loss {:.3f}; accuracy {:.3f}%; time {:.2f}s'.format(\n",
    "          validation_loss, validation_accuracy, validation_time))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best accuracy\n",
    "best_accuracy = max(validation_accuracies)\n",
    "best_model_idx = validation_accuracies.index(best_accuracy)\n",
    "print('Best accuracy on validation ds: {:.3}% obtained at the {}th iteration'\n",
    "      .format(best_accuracy, best_model_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the training convergence\n",
    "# Merge plots into one plot with two subplots\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# First subplot for training metrics\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_losses, label='Loss', color='blue')\n",
    "plt.plot(np.array(train_accuracies)/100, label='Accuracy', color='orange')\n",
    "plt.ylabel('Training Metrics')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Second subplot for validation metrics\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(validation_losses, label='Loss', color='blue')\n",
    "plt.plot(np.array(validation_accuracies)/100, label='Accuracy', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Metrics')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model_state_dicts[best_model_idx]\n",
    "torch.save(best_model, r'C:\\Users\\giuseppe.dambruoso\\OneDrive - LUTECH SPA\\Desktop\\Progetto\\BestModel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
