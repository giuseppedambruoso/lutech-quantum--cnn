{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision.transforms import Compose, Resize, Grayscale, ToTensor\n",
    "from torch.nn import Sequential, Conv2d, BatchNorm2d, ReLU, MaxPool2d, functional\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.primitives import BackendEstimator, BackendSampler\n",
    "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN, SamplerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "\n",
    "# Define constants\n",
    "IMG_SIZE = (160, 160)  # Image size for resizing\n",
    "NUM_CLASSES = 4  # Number of classes in the dataset\n",
    "BATCH_SIZE = 32  # Batch size for data loading\n",
    "\n",
    "def scale_image(image):\n",
    "    # Scale the data so that they are between 0 and pi and their quantum encoding is unique\n",
    "    image = np.array(image.squeeze())\n",
    "    scaler = StandardScaler()\n",
    "    scaled_img= scaler.fit_transform(image)\n",
    "    scaled_img = scaled_img/max(scaled_img.flatten()) *np.pi/2 + np.pi/2\n",
    "    \n",
    "    return torch.tensor(scaled_img).unsqueeze(dim=0)\n",
    "\n",
    "# Define the training dataset with ImageFolder, which assumes that the images are organized in class-specific folders\n",
    "train_ds = ImageFolder(\n",
    "    root=r\"C:\\Users\\giuseppe.dambruoso\\OneDrive - LUTECH SPA\\Desktop\\Progetto\\Dataset\\train\",  # Path to the training data directory\n",
    "    transform=Compose([  # Compose a sequence of transformations to apply to each image\n",
    "        Resize(IMG_SIZE),  # Resize the image to IMG_SIZE\n",
    "        Grayscale(num_output_channels=1),  # Convert the image to grayscale (single channel)\n",
    "        ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    ]),\n",
    "    target_transform=Compose([  # Compose a sequence of transformations to apply to each target\n",
    "        lambda x: torch.tensor(x),  # Convert the target to a PyTorch tensor\n",
    "        lambda x: torch.eye(NUM_CLASSES)[x].to(torch.float64)  # One-hot encode the target labels \n",
    "    ])\n",
    ")\n",
    "\n",
    "# Create a data loader for the training dataset, which loads data in batches\n",
    "train_loader = DataLoader(\n",
    "    train_ds,  # Training dataset\n",
    "    batch_size=BATCH_SIZE,  # Batch size for loading data\n",
    "    shuffle=True,  # Shuffle the data at every epoch\n",
    "    drop_last=True  # Drop the last incomplete batch if its size is less than BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Define the validation dataset with ImageFolder, which assumes that the images are organized in class-specific folders\n",
    "validation_ds = ImageFolder(\n",
    "    root=r\"C:\\Users\\giuseppe.dambruoso\\OneDrive - LUTECH SPA\\Desktop\\Progetto\\Dataset\\test\",  # Path to the validation data directory\n",
    "    transform=Compose([  # Compose a sequence of transformations to apply to each image\n",
    "        Resize(IMG_SIZE),  # Resize the image to IMG_SIZE\n",
    "        Grayscale(num_output_channels=1),  # Convert the image to grayscale (single channel)\n",
    "        ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "        lambda x: scale_image(x)  # Rescale values so that they are between 0 and pi\n",
    "    ]),\n",
    "    target_transform=Compose([  # Compose a sequence of transformations to apply to each target\n",
    "        lambda x: torch.tensor(x),  # Convert the target to a PyTorch tensor\n",
    "        lambda x: torch.eye(NUM_CLASSES)[x].to(torch.float64) # One-hot encode the target labels\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Create a data loader for the validation dataset, which loads data in batches\n",
    "validation_loader = DataLoader(\n",
    "    validation_ds,  # Validation dataset\n",
    "    batch_size=BATCH_SIZE,  # Batch size for loading data\n",
    "    shuffle=True,  # Shuffle the data at every epoch\n",
    "    drop_last=True  # Drop the last incomplete batch if its size is less than BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classical blocks\n",
    "def ConvBlock(in_channels, out_channels):\n",
    "    \"\"\"\n",
    "    Helper function to create a convolutional block.\n",
    "\n",
    "    Parameters:\n",
    "        - in_channels: Number of input channels.\n",
    "        - out_channels: Number of output channels.\n",
    "        - kernel_size: Size of the convolutional kernel.\n",
    "        - padding: Padding type for the convolution.\n",
    "\n",
    "    Returns:\n",
    "        - model_cb: Convolutional block as a Sequential module.\n",
    "    \"\"\"\n",
    "    model_cb = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding='same'\n",
    "        ),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.BatchNorm2d(out_channels),\n",
    "        torch.nn.MaxPool2d(kernel_size=2)\n",
    "    )\n",
    "    return model_cb\n",
    "\n",
    "def DenseBlock(in_features, out_features):\n",
    "    \"\"\"\n",
    "    Helper function to create a dense block.\n",
    "\n",
    "    Parameters:\n",
    "        - in_features: Number of input units.\n",
    "        - out_fetures: Number of output units.\n",
    "\n",
    "    Returns:\n",
    "        - model_db: Dense block as a Sequential module.\n",
    "    \"\"\"\n",
    "    model_db = torch.nn.Sequential(\n",
    "        torch.nn.Linear(\n",
    "            in_features,\n",
    "            out_features\n",
    "        ),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.BatchNorm1d(out_features),\n",
    "        torch.nn.Dropout(p=0.2)\n",
    "    )\n",
    "    return model_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parametrized quantum circuit\n",
    "N_QUBITS = 4\n",
    "SHOTS = 1000\n",
    "entanglement='linear'\n",
    "backend=Aer.get_backend('qasm_simulator')\n",
    "\n",
    "feature_map = ZZFeatureMap(N_QUBITS, parameter_prefix='x')\n",
    "ansatz = RealAmplitudes(N_QUBITS, reps=N_QUBITS, entanglement=entanglement)\n",
    "pqc = QuantumCircuit(N_QUBITS)\n",
    "pqc.compose(feature_map, inplace=True)\n",
    "pqc.compose(ansatz, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the quantum convolution\n",
    "def apply_quantum_filter(input, backend, shots):\n",
    "    quantum_filter = EstimatorQNN(\n",
    "                    circuit=pqc.decompose(), # Set quantum circuit\n",
    "                    estimator=BackendEstimator(backend), # Set sampler for the quantum circuit execution\n",
    "                    input_params=feature_map.parameters, # Set non-trainable input parameters\n",
    "                    weight_params=ansatz.parameters, # Set trainable parameters\n",
    "                    input_gradients=True\n",
    "                )\n",
    "    \n",
    "    quantum_filter.estimator.set_options(shots=shots) # Set number of shots for the quantum circuit execution\n",
    "      \n",
    "    quantum_filter = TorchConnector(quantum_filter)\n",
    "\n",
    "    item = quantum_filter(input.reshape(-1)).item()\n",
    "    return item\n",
    "          \n",
    "def quantum_convolution(input, backend, shots):\n",
    "    input_unfolded = torch.nn.functional.unfold(input, kernel_size=2, stride=1, padding=1).permute(0, 2, 1)\n",
    "\n",
    "    output_unfolded = [[] for _ in range(input_unfolded.size(0))]\n",
    "\n",
    "    for i in range(input_unfolded.size(0)):\n",
    "        for j in range(input_unfolded.size(1)):\n",
    "            output_unfolded[i].append(apply_quantum_filter(input_unfolded[i, j], backend, shots))\n",
    "    \n",
    "    output_unfolded = torch.tensor(output_unfolded)\n",
    "\n",
    "    output = output_unfolded.view(output_unfolded.size(0), input.size(1), int(sqrt(input_unfolded.size(1))), int(sqrt(input_unfolded.size(1))))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the net\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, backend, shots):\n",
    "        super(Net, self).__init__()\n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "\n",
    "        self.cnn = torch.nn.Sequential(\n",
    "            ConvBlock(1, 256),\n",
    "            ConvBlock(256, 128),\n",
    "            ConvBlock(128, 64),\n",
    "            torch.nn.Flatten(),\n",
    "            DenseBlock(64*(IMG_SIZE[0]//8)**2, 128),\n",
    "            DenseBlock(128, 64),\n",
    "            torch.nn.Linear(64,4)\n",
    "        )\n",
    "\n",
    "        self.quant=TorchConnector(SamplerQNN(\n",
    "                circuit=pqc.decompose(),\n",
    "                sampler=BackendSampler(backend),\n",
    "                input_params=feature_map.parameters,\n",
    "                weight_params=ansatz.parameters,\n",
    "                interpret=lambda x: bin(x).count(\"1\") % 2,\n",
    "                output_shape=NUM_CLASSES\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = quantum_convolution(x, backend=self.backend, shots=self.shots)\n",
    "        x = self.cnn(x)      \n",
    "        x = self.quant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the net\n",
    "model = Net(backend, SHOTS)\n",
    "\n",
    "# perform training and validation\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "TRAIN: loss 1.6183740459382534; accuracy 3.125%; time 0min\n",
      "VALIDATION: loss 1.5953592661386118; accuracy 8.333333333333332%; time 4min\n",
      "\n",
      "EPOCH 2:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[0;32m     41\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 43\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Perform one training epoch and calculate the training loss\u001b[39;00m\n\u001b[0;32m     44\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())  \u001b[38;5;66;03m# Append the training loss to the list\u001b[39;00m\n\u001b[0;32m     45\u001b[0m train_accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy) \u001b[38;5;66;03m# Append the training accuracy to the list\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[38;5;66;03m# where inputs are the images of a batch and labels are their labels\u001b[39;00m\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Zero your gradients for every batch!\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n",
      "File \u001b[1;32mc:\\Users\\giuseppe.dambruoso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giuseppe.dambruoso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# x = quantum_convolution(x, backend=self.backend, shots=self.shots)\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m      \n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(x)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\giuseppe.dambruoso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giuseppe.dambruoso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giuseppe.dambruoso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\giuseppe.dambruoso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giuseppe.dambruoso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giuseppe.dambruoso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\giuseppe.dambruoso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giuseppe.dambruoso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giuseppe.dambruoso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giuseppe.dambruoso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# perform training and validation\n",
    "def train_one_epoch():\n",
    "    for batch_index, data in enumerate(train_loader):      \n",
    "        inputs, labels = data # Every data instance is an inputs + labels pair\n",
    "                            # where inputs are the images of a batch and labels are their labels\n",
    "        \n",
    "        optimizer.zero_grad() # Zero your gradients for every batch!\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct_predictions = 0\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        _, true_labels = torch.max(labels, 1)  # Convert one-hot labels to indices\n",
    "        correct_predictions += (predicted_labels == true_labels).sum().item()\n",
    "        accuracy = correct_predictions / BATCH_SIZE * 100\n",
    "    return loss, accuracy\n",
    "\n",
    "EPOCHS = 30  # Define the total number of epochs\n",
    "train_losses = []  # Create an empty list to store training losses for each epoch\n",
    "train_accuracies = [] # Create an empty list to store training accuracies for each epoch\n",
    "validation_losses = []  # Create an empty list to store validation losses for each epoch\n",
    "validation_accuracies = [] # Create an empty list to store validation accuracies for each epoch\n",
    "model_state_dicts = [] # Create an empty list to store model state dicts for each epoch\n",
    "best_vloss = 1000  # Initialize the best validation loss with a high value\n",
    "\n",
    "for epoch in range(EPOCHS):  # Iterate over each epoch\n",
    "\n",
    "    # Perform training\n",
    "    print('EPOCH {}:'.format(epoch + 1))  # Print the current epoch number\n",
    "    \n",
    "    start_time = time.time()  # Record the start time of the epoch\n",
    "    model.train(True)  # Set the model to training mode\n",
    "    end_time = time.time()\n",
    "\n",
    "    loss, accuracy = train_one_epoch()  # Perform one training epoch and calculate the training loss\n",
    "    train_losses.append(loss.item())  # Append the training loss to the list\n",
    "    train_accuracies.append(accuracy) # Append the training accuracy to the list\n",
    "    model_state_dicts.append(model.state_dict()) # Append the model state dict to the list\n",
    "\n",
    "    print('TRAIN: loss {}; accuracy {}%; time {:.0f}min'.format(loss, accuracy,(end_time-start_time)/60))\n",
    "    \n",
    "    # Perform validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    total_vloss = 0.0  # Initialize the running validation loss\n",
    "\n",
    "    end_time = time.time()  # Record the end time of the training epoch\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation and reduce memory consumption during validation\n",
    "        start_vtime = time.time()  # Record the start time of validation\n",
    "        \n",
    "        correct_vpredictions = 0\n",
    "        \n",
    "        for batch_vindex, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            total_vloss += loss_fn(voutputs, vlabels).item()\n",
    "            _, predicted_vlabels = torch.max(voutputs, 1)\n",
    "            _, true_vlabels = torch.max(vlabels, 1)  # Convert one-hot labels to indices\n",
    "            correct_vpredictions += (predicted_vlabels == true_vlabels).sum().item()\n",
    "        \n",
    "    end_vtime = time.time()  # Record the end time of validation\n",
    "    \n",
    "    vloss = total_vloss / (batch_vindex + 1)  # Calculate the average validation loss\n",
    "    vaccuracy = correct_vpredictions / (BATCH_SIZE * (batch_vindex + 1)) * 100 # Calculate the accuracy\n",
    "    validation_losses.append(vloss)  # Append the average validation loss to the list\n",
    "    validation_accuracies.append(vaccuracy) # Append validation accuracy\n",
    "    \n",
    "    print('VALIDATION: loss {}; accuracy {}%; time {:.0f}min'.format(vloss, vaccuracy, (end_vtime-start_vtime)/60))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
