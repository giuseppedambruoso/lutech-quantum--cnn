{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import qiskit\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, CrossEntropyLoss, MSELoss\n",
    "from torch.autograd import Function \n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Grayscale\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap, ZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit.primitives import Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "\n",
    "IMG_SIZE = (160, 160)\n",
    "NUM_CLASSES = 4\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for transforming labels into one-hot encoded vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the CustomDataset class.\n",
    "\n",
    "        Parameters:\n",
    "            - dataset (torchvision.datasets.ImageFolder): The original dataset.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves the item at the specified index.\n",
    "\n",
    "        Parameters:\n",
    "            - index (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            - image (torch.Tensor): The image tensor.\n",
    "            - one_hot_label (torch.Tensor): One-hot encoded label tensor.\n",
    "        \"\"\"\n",
    "        image, label = self.dataset[index]\n",
    "        one_hot_label = torch.eye(NUM_CLASSES)[label]\n",
    "        return image, one_hot_label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            - len (int): Length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Training dataset   \n",
    "train_ds = datasets.ImageFolder(root=r\"C:\\Users\\giuseppe.dambruoso\\OneDrive - LUTECH SPA\\Desktop\\Progetto\\Dataset\\train\",\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.Resize(IMG_SIZE),\n",
    "                                    transforms.Grayscale(num_output_channels=1),\n",
    "                                    transforms.ToTensor()\n",
    "                                ]))\n",
    "train_ds = CustomDataset(train_ds) # Transform the original training dataset into a custom dataset with one-hot encoded labels\n",
    "# train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "# train_subset_size = 96\n",
    "# indices = torch.randperm(len(train_ds))[:train_subset_size]\n",
    "# subset_sampler = SubsetRandomSampler(indices)\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_ds,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     sampler=subset_sampler\n",
    "# )\n",
    "\n",
    "# Validation dataset\n",
    "validation_ds = datasets.ImageFolder(root=r\"C:\\Users\\giuseppe.dambruoso\\OneDrive - LUTECH SPA\\Desktop\\Progetto\\Dataset\\test\",\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.Resize(IMG_SIZE),\n",
    "                                    transforms.Grayscale(num_output_channels=1),\n",
    "                                    transforms.ToTensor()\n",
    "                                ]))\n",
    "validation_ds = CustomDataset(validation_ds) # Transform the original testing dataset into a custom dataset with one-hot encoded labels\n",
    "# validation_loader = torch.utils.data.DataLoader(validation_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "# validation_subset_size = 32\n",
    "# indices = torch.randperm(len(validation_ds))[:validation_subset_size]\n",
    "# subset_sampler = SubsetRandomSampler(indices)\n",
    "# validation_loader = torch.utils.data.DataLoader(\n",
    "#     validation_ds,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     sampler=subset_sampler\n",
    "# )\n",
    "\n",
    "# Combine the training and validation datasets\n",
    "# combined_ds = ConcatDataset([train_ds, validation_ds])\n",
    "\n",
    "# Define the sizes for the training and validation subsets\n",
    "train_size = len(train_ds)\n",
    "validation_size = len(validation_ds)\n",
    "\n",
    "# Split the combined dataset into training and validation subsets\n",
    "# train_ds, validation_ds = random_split(combined_ds, [train_size, validation_size])\n",
    "\n",
    "# Create data loaders for the training and validation subsets\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classical layers\n",
    "def ConvLayer(in_channels, out_channels, kernel_size, padding='same'):\n",
    "    \"\"\"\n",
    "    Helper function to create a convolutional block.\n",
    "\n",
    "    Parameters:\n",
    "        - in_channels: Number of input channels.\n",
    "        - out_channels: Number of output channels.\n",
    "        - kernel_size: Size of the convolutional kernel.\n",
    "        - padding: Padding type for the convolution.\n",
    "\n",
    "    Returns:\n",
    "        - model_cb: Convolutional block as a Sequential module.\n",
    "    \"\"\"\n",
    "    model_cb = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2)\n",
    "    )\n",
    "    return model_cb\n",
    "\n",
    "def DenseLayer(in_units, out_units):\n",
    "    \"\"\"\n",
    "    Helper function to create a dense block.\n",
    "\n",
    "    Parameters:\n",
    "        - in_units: Number of input units.\n",
    "        - out_units: Number of output units.\n",
    "\n",
    "    Returns:\n",
    "        - model_db: Dense block as a Sequential module.\n",
    "    \"\"\"\n",
    "    model_db = nn.Sequential(\n",
    "        nn.Linear(in_units, out_units),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(out_units),\n",
    "        nn.Dropout(p=0.2)\n",
    "    )\n",
    "    return model_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the quantum layer\n",
    "def create_quantum_layer(num_inputs, num_outputs, feature_map, quantum_layer, shots):\n",
    "    feature_map = feature_map(num_inputs) # create a ZZFeatureMap\n",
    "    ansatz = RealAmplitudes(num_inputs, reps=1) # create a RealAmplitudes map with one repetition\n",
    "\n",
    "    quantum_circuit = QuantumCircuit(num_inputs) # create a quantum circuit\n",
    "    quantum_circuit.compose(feature_map, inplace=True) # apply the feature map\n",
    "    quantum_circuit.compose(ansatz, inplace=True) # apply the ansatz\n",
    "\n",
    "    if quantum_layer == EstimatorQNN:\n",
    "        quantum_layer = EstimatorQNN(\n",
    "                            circuit=quantum_circuit,\n",
    "                            input_params=feature_map.parameters,\n",
    "                            weight_params=ansatz.parameters,\n",
    "                            input_gradients=True,\n",
    "                        )\n",
    "        quantum_layer.estimator.set_options(shots=shots)\n",
    "    \n",
    "    if quantum_layer == SamplerQNN:\n",
    "        parity = lambda x: \"{:b}\".format(x).count(\"1\") % 2  # optional interpret function\n",
    "        quantum_layer = SamplerQNN(\n",
    "                            circuit=quantum_circuit,\n",
    "                            input_params=feature_map.parameters,\n",
    "                            weight_params=ansatz.parameters,\n",
    "                            output_shape=num_outputs,\n",
    "                            interpret=parity,\n",
    "                        )\n",
    "        quantum_layer.sampler.set_options(shots=shots)\n",
    "    \n",
    "    return quantum_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the net\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "        Neural network model composed of convolutional, dense and quantum layers.\n",
    "\n",
    "        Architecture:\n",
    "        - Three convolutional layers: ConvLayer(1, 256), ConvLayer(256, 128), ConvLayer(128, 64)\n",
    "        - Two fully connected layers: DenseLayer(25600, 128), DenseLayer(128, 64)\n",
    "        - Output layer: nn.Linear(64, 4)\n",
    "        - Quantum layer: QuantumLayer(backend, 100, np.pi/2)\n",
    "\n",
    "        Parameters:\n",
    "            - None\n",
    "\n",
    "        Attributes:\n",
    "            - conv1 (ConvLayer): First convolutional layer with input channels=1 and output channels=256.\n",
    "            - conv2 (ConvLayer): Second convolutional layer with input channels=256 and output channels=128.\n",
    "            - conv3 (ConvLayer): Third convolutional layer with input channels=128 and output channels=64.\n",
    "            - fc1 (DenseLayer): First fully connected layer with input features=25600 and output features=128.\n",
    "            - fc2 (DenseLayer): Second fully connected layer with input features=128 and output features=64.\n",
    "            - fc3 (nn.Linear): Output layer with input features=64 and output features=4.\n",
    "            - quant (QuantumLayer): Quantum layer with specified backend, shots, and shift parameters.\n",
    "\n",
    "        Methods:\n",
    "            - forward(x): Forward pass of the neural network.\n",
    "\n",
    "        Usage:\n",
    "        ```\n",
    "        net = Net()\n",
    "        output = net(input_tensor)\n",
    "        ```\n",
    "    \"\"\"\n",
    "    def __init__(self, quantum_layer):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = ConvLayer(1, 256, kernel_size=5)\n",
    "        self.conv2 = ConvLayer(256, 128, kernel_size=5)\n",
    "        self.conv3 = ConvLayer(128, 64, kernel_size=5)\n",
    "        self.fc1 = DenseLayer(25600, 128)\n",
    "        self.fc2 = DenseLayer(128, 64)\n",
    "        self.fc3 = DenseLayer(64, 4)\n",
    "        self.quant = TorchConnector(quantum_layer) # ha dimensioni 4x1 se Estimator, 4x4 se Sampler\n",
    "        self.fc4 = nn.Linear(4, 4)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.quant(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the net\n",
    "epochs = 30\n",
    "shots = 5000\n",
    "train_loss_list = []\n",
    "train_accuracy_list = []\n",
    "validation_loss_list = []\n",
    "validation_accuracy_list = []\n",
    "quantum_layer = create_quantum_layer(\n",
    "    num_inputs=4,\n",
    "    num_outputs=4,\n",
    "    feature_map=ZZFeatureMap,\n",
    "    quantum_layer=SamplerQNN,\n",
    "    shots = shots\n",
    ")\n",
    "model = Net(quantum_layer)\n",
    "LEARNING_RATE = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        output = F.softmax(model(data).squeeze(), dim=-1)\n",
    "        target = target.squeeze().type('torch.DoubleTensor')\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss.append(loss.item())\n",
    "        pred = []\n",
    "        for o in output:\n",
    "            prediction = torch.tensor(o.argmax())\n",
    "            pred.append(prediction)\n",
    "        tar = []\n",
    "        for t in target.squeeze():\n",
    "            targ = t.argmax()\n",
    "            tar.append(targ)\n",
    "        for j in range(len(pred)):\n",
    "            if pred[j] == tar[j] :\n",
    "                correct += 1\n",
    "    train_epoch_accuracy = correct / (len(train_loader)*BATCH_SIZE) * 100\n",
    "    train_epoch_loss = sum(total_loss) / len(total_loss)\n",
    "    train_accuracy_list.append(train_epoch_accuracy)\n",
    "    train_loss_list.append(train_epoch_loss)\n",
    "    end_time = time.time()\n",
    "    epoch_time = (end_time - start_time)/60\n",
    "    print('Epoch: {:d}, Loss: {:.4f}, Accuracy: {:.2f}%, Time: {:.1f}min'.format(i+1, train_epoch_loss, train_epoch_accuracy, epoch_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training loss over epochs\n",
    "plt.plot(train_loss_list, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "        output = model(data).squeeze()\n",
    "        target = target.squeeze().type('torch.DoubleTensor')\n",
    "        validation_epoch_loss = loss_func(output, target)\n",
    "        pred = []\n",
    "        for o in output:\n",
    "            prediction = torch.tensor(o.argmax())\n",
    "            pred.append(prediction)\n",
    "        tar = []\n",
    "        for t in target:\n",
    "            targ = t.argmax()\n",
    "            tar.append(targ)\n",
    "        for l in range(len(pred)):\n",
    "            if pred[l] == tar[l] :\n",
    "                correct += 1\n",
    "    validation_epoch_accuracy = correct/(len(validation_loader)*BATCH_SIZE)*100\n",
    "    print('Validation - Loss: {:.4f}, Accuracy: {:.1f}%'.format(validation_epoch_loss, validation_epoch_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\giuseppe.dambruoso\\OneDrive - LUTECH SPA\\Desktop\\Progetto\\Risultati\\trained_HQCCNN_fc3_shuffled'\n",
    "torch.save(model.state_dict(), path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
